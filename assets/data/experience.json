[
  {
    "period": "Nov 2024 - Present",
    "role": "Senior Data Engineer - Asset Custody Services",
    "company": "Itaú Unibanco",
    "logo": "assets/images/itau_logo.jpg",
    "description": "",
    "highlights": [
    ],
    "technologies": ["AWS", "GCP", "Python", "Terraform", "Spark", "Airflow"]
  },
  {
    "period": "Jun 2023 - Oct 2024",
    "role": "Senior Data Engineer - Fraud Prevention",
    "company": "Itaú Unibanco",
    "logo": "assets/images/itau_logo.jpg",
    "description": "Developed and maintained data pipelines using Apache Airflow, Spark, and Kafka. Built real-time streaming solutions for high-volume data processing. Collaborated with data scientists to implement ML models in production environments.",
    "highlights": [
      "Architected real-time streaming pipeline processing 50M+ events daily",
      "Reduced data processing time from hours to seconds",
      "Built data quality monitoring framework with automated alerts"
    ],
    "technologies": ["Apache Kafka", "PySpark", "Airflow", "Python", "SQL", "Docker"]
  },
  {
    "period": "Nov 2022 - Jun 2023",
    "role": "Senior Data Engineer",
    "company": "Banco Inter",
    "logo": "assets/images/inter_logo.jpg",
    "description": "Developed and maintained data pipelines using Apache Airflow, Spark, and Kafka. Built real-time streaming solutions for high-volume data processing. Collaborated with data scientists to implement ML models in production environments.",
    "highlights": [
      "Architected real-time streaming pipeline processing 50M+ events daily",
      "Reduced data processing time from hours to seconds",
      "Built data quality monitoring framework with automated alerts"
    ],
    "technologies": ["Apache Kafka", "PySpark", "Airflow", "Python", "SQL", "Docker"]
  },
  {
    "period": "Jan 2022 - Nov 2022",
    "role": "Data Engineer",
    "company": "Banco Inter",
    "logo": "assets/images/inter_logo.jpg",
    "description": "Developed and maintained data pipelines using Apache Airflow, Spark, and Kafka. Built real-time streaming solutions for high-volume data processing. Collaborated with data scientists to implement ML models in production environments.",
    "highlights": [
      "Architected real-time streaming pipeline processing 50M+ events daily",
      "Reduced data processing time from hours to seconds",
      "Built data quality monitoring framework with automated alerts"
    ],
    "technologies": ["Apache Kafka", "PySpark", "Airflow", "Python", "SQL", "Docker"]
  },
  {
    "period": "Jan 2021 - Jan 2022",
    "role": "Junior Data Engineer",
    "company": "Banco Inter",
    "logo": "assets/images/inter_logo.jpg",
    "description": "Developed and maintained data pipelines using Apache Airflow, Spark, and Kafka. Built real-time streaming solutions for high-volume data processing. Collaborated with data scientists to implement ML models in production environments.",
    "highlights": [
      "Architected real-time streaming pipeline processing 50M+ events daily",
      "Reduced data processing time from hours to seconds",
      "Built data quality monitoring framework with automated alerts"
    ],
    "technologies": ["Apache Kafka", "PySpark", "Airflow", "Python", "SQL", "Docker"]
  },
  {
    "period": "Mar 2020 - Jan 2021",
    "role": "Intern Data Engineer",
    "company": "Opala Studios",
    "logo": "assets/images/opalastudios_logo.jpg",
    "description": "Developed and maintained data pipelines using Apache Airflow, Spark, and Kafka. Built real-time streaming solutions for high-volume data processing. Collaborated with data scientists to implement ML models in production environments.",
    "highlights": [
      "Architected real-time streaming pipeline processing 50M+ events daily",
      "Reduced data processing time from hours to seconds",
      "Built data quality monitoring framework with automated alerts"
    ],
    "technologies": ["Apache Kafka", "PySpark", "Airflow", "Python", "SQL", "Docker"]
  },
  {
    "period": "Apr 2019 - Mar 2020",
    "role": "Python Developer",
    "company": "Universidade Federal do Paraná",
    "logo": "assets/images/ufpr_logo.jpg",
    "description": "Developed and maintained data pipelines using Apache Airflow, Spark, and Kafka. Built real-time streaming solutions for high-volume data processing. Collaborated with data scientists to implement ML models in production environments.",
    "highlights": [
      "Architected real-time streaming pipeline processing 50M+ events daily",
      "Reduced data processing time from hours to seconds",
      "Built data quality monitoring framework with automated alerts"
    ],
    "technologies": ["Apache Kafka", "PySpark", "Airflow", "Python", "SQL", "Docker"]
  }
] 