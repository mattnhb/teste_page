[
  {
    "period": "Nov 2024 - Present",
    "role": "Senior Data Engineer - Asset Custody Services",
    "company": "Itaú Unibanco",
    "logo": "assets/images/itau_logo.jpg",
    "description": "Leading a team of data engineers in designing and implementing scalable data platforms. Architecting cloud-based data solutions on AWS and GCP, optimizing data pipelines, and establishing data governance practices. Reduced processing costs by 35% while increasing data throughput.",
    "highlights": [
      "Led a team of 5 data engineers across 3 major projects",
      "Reduced cloud infrastructure costs by 35% while improving performance",
      "Implemented data governance framework adopted company-wide"
    ],
    "technologies": ["AWS", "GCP", "Python", "Terraform", "Spark", "Airflow"]
  },
  {
    "period": "Jun 2023 - Oct 2024",
    "role": "Senior Data Engineer - Fraud Prevention",
    "company": "Itaú Unibanco",
    "logo": "assets/images/itau_logo.jpg",
    "description": "Developed and maintained data pipelines using Apache Airflow, Spark, and Kafka. Built real-time streaming solutions for high-volume data processing. Collaborated with data scientists to implement ML models in production environments.",
    "highlights": [
      "Architected real-time streaming pipeline processing 50M+ events daily",
      "Reduced data processing time from hours to seconds",
      "Built data quality monitoring framework with automated alerts"
    ],
    "technologies": ["Apache Kafka", "PySpark", "Airflow", "Python", "SQL", "Docker"]
  },
  {
    "period": "Nov 2022 - Jun 2023",
    "role": "Senior Data Engineer",
    "company": "Banco Inter",
    "logo": "assets/images/inter_logo.jpg",
    "description": "Developed and maintained data pipelines using Apache Airflow, Spark, and Kafka. Built real-time streaming solutions for high-volume data processing. Collaborated with data scientists to implement ML models in production environments.",
    "highlights": [
      "Architected real-time streaming pipeline processing 50M+ events daily",
      "Reduced data processing time from hours to seconds",
      "Built data quality monitoring framework with automated alerts"
    ],
    "technologies": ["Apache Kafka", "PySpark", "Airflow", "Python", "SQL", "Docker"]
  },
  {
    "period": "Jan 2022 - Nov 2022",
    "role": "Data Engineer",
    "company": "Banco Inter",
    "logo": "assets/images/inter_logo.jpg",
    "description": "Developed and maintained data pipelines using Apache Airflow, Spark, and Kafka. Built real-time streaming solutions for high-volume data processing. Collaborated with data scientists to implement ML models in production environments.",
    "highlights": [
      "Architected real-time streaming pipeline processing 50M+ events daily",
      "Reduced data processing time from hours to seconds",
      "Built data quality monitoring framework with automated alerts"
    ],
    "technologies": ["Apache Kafka", "PySpark", "Airflow", "Python", "SQL", "Docker"]
  },
  {
    "period": "Jan 2021 - Jan 2022",
    "role": "Junior Data Engineer",
    "company": "Banco Inter",
    "logo": "assets/images/inter_logo.jpg",
    "description": "Developed and maintained data pipelines using Apache Airflow, Spark, and Kafka. Built real-time streaming solutions for high-volume data processing. Collaborated with data scientists to implement ML models in production environments.",
    "highlights": [
      "Architected real-time streaming pipeline processing 50M+ events daily",
      "Reduced data processing time from hours to seconds",
      "Built data quality monitoring framework with automated alerts"
    ],
    "technologies": ["Apache Kafka", "PySpark", "Airflow", "Python", "SQL", "Docker"]
  },
  {
    "period": "Mar 2020 - Jan 2021",
    "role": "Intern Data Engineer",
    "company": "Opala Studios",
    "logo": "assets/images/opalastudios_logo.jpg",
    "description": "Developed and maintained data pipelines using Apache Airflow, Spark, and Kafka. Built real-time streaming solutions for high-volume data processing. Collaborated with data scientists to implement ML models in production environments.",
    "highlights": [
      "Architected real-time streaming pipeline processing 50M+ events daily",
      "Reduced data processing time from hours to seconds",
      "Built data quality monitoring framework with automated alerts"
    ],
    "technologies": ["Apache Kafka", "PySpark", "Airflow", "Python", "SQL", "Docker"]
  },
  {
    "period": "Apr 2019 - Mar 2020",
    "role": "Python Developer",
    "company": "Universidade Federal do Paraná",
    "logo": "assets/images/ufpr_logo.jpg",
    "description": "Developed and maintained data pipelines using Apache Airflow, Spark, and Kafka. Built real-time streaming solutions for high-volume data processing. Collaborated with data scientists to implement ML models in production environments.",
    "highlights": [
      "Architected real-time streaming pipeline processing 50M+ events daily",
      "Reduced data processing time from hours to seconds",
      "Built data quality monitoring framework with automated alerts"
    ],
    "technologies": ["Apache Kafka", "PySpark", "Airflow", "Python", "SQL", "Docker"]
  }
] 